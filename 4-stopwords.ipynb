{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a759e39",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "#### In the below paragraph we have words like \"to\", \"the\", \"of\" which are not useful for analysis.\n",
    "#### We will remove these stop words using nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\" At its core, NLP is a branch of artificial intelligence that focuses on enabling computers to understand,\n",
    "interpret, and respond to human language in a meaningful way. Whether it's text or speech, \n",
    "NLP gives machines the ability to interact with us just like another human might.\n",
    "Think about how you use Siri, Google Assistant, or Alexa. Ever typed a sentence into Google Translate or spoken a message that was transcribed into text? All of this is made possible through NLP.\n",
    "So, how does it work?\n",
    "NLP combines linguistics, machine learning, and statistics. It breaks down language into parts—like tokens, syntax, semantics, and context—so computers can analyze them.\n",
    "Tools like NLTK, spaCy, and models like GPT (yes, the same tech powering ChatGPT!) are used for tasks such as translation, summarization, and answering questions.\n",
    "Of course, NLP isn’t perfect. Human language is complex—it’s full of sarcasm, slang, cultural nuances, and ambiguity. But with advances in deep learning and big data, NLP is rapidly improving.\n",
    "In the coming years, NLP will become even more integrated into our lives—powering better communication tools, smarter assistants, and more intuitive ways of interacting with technology.\n",
    "To conclude, NLP is not just about teaching machines how to read or talk—it’s about bridging the gap between humans and machines, making technology feel more natural and human-centric.\n",
    "Thank you!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing the necessary library\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dbe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/udmdev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Downloading the stopwords dataset\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dccda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## There will be different languages stopwords available in nltk library.\n",
    "## Here you also can create your own list of stopwords., \n",
    "## in the below words, few words like not, we can't remove this kind of words, it represnet the negative meaning.\n",
    "## Let's see what are the stopwords available in nltk library.\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e08a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'dass',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Like as said before we have different languages stopwords available in nltk library.\n",
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d725d",
   "metadata": {},
   "source": [
    "Now will apply all differnet kind of steeming and lemmatization and check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3cfada",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First will apply stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6184804",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab2d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eebafdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's traverse through all the sentences, first apply a stopwords, \n",
    "# and whichever words are not present in the stop words, will take that and apply stemming\n",
    "\n",
    "\n",
    "## Apply stowards and filter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # using set to remove duplicates\n",
    "    sentences[i] = ' '.join(words) # Converting all the words into a sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282c50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['at core , nlp branch artifici intellig focus enabl comput understand , interpret , respond human languag meaning way .',\n",
       " \"whether 's text speech , nlp give machin abil interact us like anoth human might .\",\n",
       " 'think use siri , googl assist , alexa .',\n",
       " 'ever type sentenc googl translat spoken messag transcrib text ?',\n",
       " 'all made possibl nlp .',\n",
       " 'so , work ?',\n",
       " 'nlp combin linguist , machin learn , statist .',\n",
       " 'it break languag parts—lik token , syntax , semant , context—so comput analyz .',\n",
       " 'tool like nltk , spaci , model like gpt ( ye , tech power chatgpt ! )',\n",
       " 'use task translat , summar , answer question .',\n",
       " 'of cours , nlp ’ perfect .',\n",
       " 'human languag complex—it ’ full sarcasm , slang , cultur nuanc , ambigu .',\n",
       " 'but advanc deep learn big data , nlp rapidli improv .',\n",
       " 'in come year , nlp becom even integr lives—pow better commun tool , smarter assist , intuit way interact technolog .',\n",
       " 'to conclud , nlp teach machin read talk—it ’ bridg gap human machin , make technolog feel natur human-centr .',\n",
       " 'thank !']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Now we can see the senetences in which stopwords are removed and stemming is applied\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83e61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will apply snowball stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowballStemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e62c6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply snowball stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowballStemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # using set to remove duplicates\n",
    "    sentences[i] = ' '.join(words) # Converting all the words into a sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac5b3069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['core , nlp branch artifici intellig focus enabl comput understand , interpret , respond human languag mean way .',\n",
       " \"whether 's text speech , nlp give machin abil interact us like anoth human might .\",\n",
       " 'think use siri , googl assist , alexa .',\n",
       " 'ever type sentenc googl translat spoken messag transcrib text ?',\n",
       " 'made possibl nlp .',\n",
       " ', work ?',\n",
       " 'nlp combin linguist , machin learn , statist .',\n",
       " 'break languag parts—lik token , syntax , semant , context—so comput analyz .',\n",
       " 'tool like nltk , spaci , model like gpt ( ye , tech power chatgpt ! )',\n",
       " 'use task translat , summar , answer question .',\n",
       " 'cour , nlp ’ perfect .',\n",
       " 'human languag complex—it ’ full sarcasm , slang , cultur nuanc , ambigu .',\n",
       " 'advanc deep learn big data , nlp rapid improv .',\n",
       " 'come year , nlp becom even integr lives—pow better commun tool , smarter assist , intuit way interact technolog .',\n",
       " 'conclud , nlp teach machin read talk—it ’ bridg gap human machin , make technolog feel natur human-centr .',\n",
       " 'thank !']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Will aplly lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7debd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].lower() # Converting all the letters into small\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in set(stopwords.words('english'))] # using set to remove duplicates\n",
    "    sentences[i] = ' '.join(words) # Converting all the words into a sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd90f68a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['core , nlp branch artifici intellig focus enabl comput understand , interpret , respond human languag mean way .',\n",
       " \"whether 's text speech , nlp give machin abil interact us like anoth human might .\",\n",
       " 'think use siri , googl assist , alexa .',\n",
       " 'ever type sentenc googl translat speak messag transcrib text ?',\n",
       " 'make possibl nlp .',\n",
       " ', work ?',\n",
       " 'nlp combin linguist , machin learn , statist .',\n",
       " 'break languag parts—lik token , syntax , semant , context—so comput analyz .',\n",
       " 'tool like nltk , spaci , model like gpt ( ye , tech power chatgpt ! )',\n",
       " 'use task translat , summar , answer question .',\n",
       " 'cour , nlp ’ perfect .',\n",
       " 'human languag complex—it ’ full sarcasm , slang , cultur nuanc , ambigu .',\n",
       " 'advanc deep learn big data , nlp rapid improv .',\n",
       " 'come year , nlp becom even integr lives—pow better commun tool , smarter assist , intuit way interact technolog .',\n",
       " 'conclud , nlp teach machin read talk—it ’ bridg gap human machin , make technolog feel natur human-centr .',\n",
       " 'thank !']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
