{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\" At its core, NLP is a branch of artificial intelligence that focuses on enabling computers to understand,\n",
    "interpret, and respond to human language in a meaningful way. Whether it's text or speech, \n",
    "NLP gives machines the ability to interact with us just like another human might.\n",
    "Think about how you use Siri, Google Assistant, or Alexa. Ever typed a sentence into Google Translate or spoken a message that was transcribed into text? All of this is made possible through NLP.\n",
    "So, how does it work?\n",
    "NLP combines linguistics, machine learning, and statistics. It breaks down language into parts—like tokens, syntax, semantics, and context—so computers can analyze them.\n",
    "Tools like NLTK, spaCy, and models like GPT (yes, the same tech powering ChatGPT!) are used for tasks such as translation, summarization, and answering questions.\n",
    "Of course, NLP isn’t perfect. Human language is complex—it’s full of sarcasm, slang, cultural nuances, and ambiguity. But with advances in deep learning and big data, NLP is rapidly improving.\n",
    "In the coming years, NLP will become even more integrated into our lives—powering better communication tools, smarter assistants, and more intuitive ways of interacting with technology.\n",
    "To conclude, NLP is not just about teaching machines how to read or talk—it’s about bridging the gap between humans and machines, making technology feel more natural and human-centric.\n",
    "Thank you!\n",
    "\"\"\"\n",
    "\n",
    "## In above paragraph we have words like \"to\", \"the\", \"of\" which are not useful for analysis.\n",
    "## We will remove these stop words using nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f0854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "## There will be different languages stopwords available in nltk library.\n",
    "## Here you also can create your own list of stopwords., in the below words few words like not, we can't remove this kind of words, it represnet the negative meaning.\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3cfada",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will apply stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6184804",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab2d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6837cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebafdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's traverse through all the sentences, first apply a stopwords, \n",
    "# and whichever words are not present in the stop words, will take that and apply stemming\n",
    "\n",
    "\n",
    "## Apply stowards and filter and then apply stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # using set to remove duplicates\n",
    "    sentences[i] = ' '.join(words) # Converting all the words into a sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a282c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e61c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will apply snowball stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowballStemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply snowball stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [snowballStemmer.stem(word) for word in words if word not in set(stopwords.words('english'))] # using set to remove duplicates\n",
    "    sentences[i] = ' '.join(words) # Converting all the words into a sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dc65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7debd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = sentences[i].lower() # Converting all the letters into small\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word, pos='v') for word in words if word not in set(stopwords.words('english'))] # using set to remove duplicates\n",
    "    sentences[i] = ' '.join(words) # Converting all the words into a sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
